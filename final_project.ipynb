{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "This final project can be collaborative. The maximum members of a group is 3. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints). You can freely determine every component of your workflow, including but not limited to:\n",
    "-  **Preprocessing the input text**: You may decide how to clean or transform the text. For example, removing emojis or URLs, lowercasing, removing stopwords, applying stemming or lemmatization, correcting spelling, or performing tokenization and sentence segmentation.\n",
    "-  **Feature extraction and encoding**: You can choose any method to convert text into numerical representations, such as TF-IDF, Bag-of-Words, N-grams, Word2Vec, GloVe, FastText, contextual embeddings (e.g., BERT, RoBERTa, or other transformer-based models), Part-of-Speech (POS) tagging, dependency-based features, sentiment or emotion features, readability metrics, or even embeddings or features generated by large language models (LLMs).\n",
    "-  **Data augmentation and enrichment**: You may expand or balance your dataset by incorporating other related corpora or using techniques like synonym replacement, random deletion/insertion, or LLM-assisted augmentation (e.g., generating paraphrased or synthetic examples to improve model robustness).\n",
    "-  **Model selection**: You are free to experiment with different models — from traditional machine learning algorithms (e.g., Logistic Regression, SVM, Random Forest, XGBoost) to deep learning architectures (e.g., CNNs, RNNs, Transformers), or even hybrid/ensemble approaches that combine multiple models or leverage LLM-generated predictions or reasoning.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values. You may explore both traditional and AI-assisted techniques. Data augmentation is optional.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision (P), Recall (R) and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. Here is an example illustrating how the experimental results table should be presented.\n",
    "\n",
    "| Feature + Model | Sexist (P) | Sexist (R) | Sexist (F1) | Non-Sexist (P) | Non-Sexist (R) | Non-Sexist (F1) | Weighted (P) | Weighted (R) | Weighted (F1) |\n",
    "|-----------------|:----------:|:----------:|:------------:|:---------------:|:---------------:|:----------------:|:-------------:|:--------------:|:---------------:|\n",
    "| TF-IDF + Logistic Regression | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "- **Format of the report**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary for each sections: \n",
    "    - Data Preprocessing\n",
    "    - Feature Engineering\n",
    "    - Model Selection and Architecture\n",
    "    - Training and Validation\n",
    "    - Evaluation and Results\n",
    "    - Use of Generative AI (if you use)\n",
    "\n",
    "## Rules \n",
    "Violations will result in 0 points in the grade: \n",
    "-   `Rule 1 - No test set leakage`: You must not use any instance from the test set during training, feature engineering, or model selection.\n",
    "-   `Rule 2 - Responsible AI use`: You may use generative AI, but you must clearly document how it was used. If you have used genAI, include a section titled “Use of Generative AI” describing:\n",
    "    -   What parts of the project you used AI for\n",
    "    -   What was implemented manually vs. with AI assistance\n",
    "\n",
    "## Grading\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above. \n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "If your best performance reaches **0.82** or above (weighted F1-score) and follows all the requirements and rules, you will also get full points (10.0 points). \n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report including: \n",
    "- code and experimental results with details explained\n",
    "- combined results table, report and best performance\n",
    "- a summary at the end of the report (please follow the format above)\n",
    "\n",
    "Missing any part of the above requirements will result in point deductions.\n",
    "\n",
    "The due date is **Dec 11, Thursday by 11:59pm**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bab11",
   "metadata": {},
   "source": [
    "### Required Libraries/Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6dfe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install the necessary libraries\n",
    "# Uncomment below if needed to do so\n",
    "#%pip install numpy\n",
    "#%pip install pandas\n",
    "#%pip install sklearn\n",
    "#%pip install scipy\n",
    "#%pip install sentence-transformers\n",
    "# Install all required libraries for the project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde634d",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "Here in data preprocessing, we are mainly grabbing the CSV data and correctly parsing it using our parse() function. Due to the text portion within the csv having commas, we must split the id from the left and the label along with split column from the right and the remaining portion would be our text portion. We then use pandas Dataframe filtering to split the data set into the training and testing sets. For text preprocessing, we decided to utilize the re library to get rid of URLs and extra spacing within text as well as Python's .lower() function. After we applied the preprocessing function to the data set. We stripped the label texts as well as put them in a LabelEncoder to ease the use for our machine learning classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194206a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# open the csv\n",
    "with open(\"edos_labelled_data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "# place into a dataframe and label them as raw\n",
    "raw = pd.DataFrame({\"raw\": lines})\n",
    "\n",
    "def parse(line):\n",
    "    # parse each line into id, text, label, split\n",
    "    line = line[\"raw\"]\n",
    "    # split from the right to get label and split\n",
    "    text_and_id_part, label, split = line.rsplit(\",\", 2)\n",
    "    # split from the left to get and text\n",
    "    id_, text = text_and_id_part.split(\",\", 1)\n",
    "    return pd.Series([id_, text, label, split])\n",
    "# get rid of the first line (header)\n",
    "raw = raw[1:]\n",
    "# apply the parse function to each row\n",
    "df = raw.apply(parse, axis=1)\n",
    "# label the columns\n",
    "df.columns = [\"id\", \"text\", \"label\", \"split\"]\n",
    "\n",
    "# split data set into train and test sets\n",
    "train_df = df[df[\"split\"] == \" train\"]\n",
    "test_df = df[df[\"split\"] == \" test\"]\n",
    "# get rid of the split column and id column\n",
    "train_df = train_df.drop(columns=[\"split\"])\n",
    "test_df = test_df.drop(columns=[\"split\"])\n",
    "\n",
    "# process text data\n",
    "def preprocess_text(text):\n",
    "    # Just lowercase and remove URLs\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "# apply preprocessing function to text data\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_text)\n",
    "# strip whitespace from labels\n",
    "train_df[\"label\"] = train_df[\"label\"].str.strip()\n",
    "test_df[\"label\"] = test_df[\"label\"].str.strip()\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df[\"label\"])\n",
    "y_test = le.transform(test_df[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bf801",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering\n",
    "For features, we used TF-IDF, which takes words and, based on how common they are in the data, changes their importance. A word like \"the\" isn't helpful since it appears in very many data points, but a word like \"bitch\" will only be in some, making it more likely to be distinctive. We also used GloVe embeddings, which uses a database in order to \"vectorize\" each word based on its semantic meaning, like \"girl\" and \"woman\", being similar, will have similar GloVe vectors. And, once you have all the vectors of each word in a sentence, they can combine to make a single vector for the entire sentence, giving its semantic meaning a number value that the models can work with. We also made a few custom features that apply to this specific case, like deragatory words like \"whore\" being weighed very very high, and checking a sentence for its female pronoun to all pronoun ratio to see if its about sex for potential sexism. All of these features are combined together to become our final feature engineering. We test them seperately as well, however, in case it ends up being more efficient. We also decided to use \"all-mpnet-base-v2\" model from contextual embeddings with the Sentence-Transformers library. After testing with these features individually, we were unable to get satisfactory results so we seeked assistance from AI to see if there was a way to combine these features to create an ultimate feature engineered set which is done by putting the data sets trained by each feature into a csr_matrix which is then combined into our TF-IDF Vectorizer using the hstack function from scipy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. TF-IDF Vectorizer - turns text into TF-IDF features\n",
    "# Weights words by importance: common words like 'the' get low weight,\n",
    "# distinctive words like 'bitch' get high weight\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=8000,     \n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    token_pattern=r'\\S+'\n",
    ")\n",
    "\n",
    "# apply vectorizer to a data set\n",
    "X_train_tfidf = tfidf.fit_transform(train_df[\"text\"])\n",
    "X_test_tfidf = tfidf.transform(test_df[\"text\"])\n",
    "\n",
    "# Select top 1800 most informative TF-IDF features\n",
    "selector_tfidf = SelectKBest(chi2, k=1800)\n",
    "X_train_tfidf = selector_tfidf.fit_transform(X_train_tfidf, y_train)\n",
    "X_test_tfidf = selector_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "# 2. GloVe embeddings (100d) - captures semantic similarity\n",
    "glove = {}\n",
    "with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        glove[word] = vector\n",
    "\n",
    "def sentence_to_vec(sentence, embeddings=glove, dim=100):\n",
    "    \"\"\"Convert sentence to vector by averaging word embeddings\"\"\"\n",
    "    words = sentence.split()\n",
    "    vectors = [embeddings[w] for w in words if w in embeddings]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_train_glove = np.vstack(train_df[\"text\"].apply(sentence_to_vec))\n",
    "X_test_glove = np.vstack(test_df[\"text\"].apply(sentence_to_vec))\n",
    "\n",
    "# 3. CONTEXTUAL EMBEDDINGS (Sentence-BERT / MPNet)\n",
    "ctx_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "X_train_ctx = ctx_model.encode(\n",
    "    train_df[\"text\"].tolist(),\n",
    "    convert_to_numpy=True,\n",
    ")\n",
    "X_test_ctx = ctx_model.encode(\n",
    "    test_df[\"text\"].tolist(),\n",
    "    convert_to_numpy=True,\n",
    ")\n",
    "\n",
    "# 4. Custom sexism-specific features\n",
    "def extract_custom_features(df):\n",
    "    features = []\n",
    "    \n",
    "    for text in df['text']:\n",
    "        feat = []\n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        # flag deragatory terms\n",
    "        derogatory = ['bitch', 'bitches', 'whore', 'whores', 'slut', 'sluts', \n",
    "                      'skank', 'skanks', 'cunt', 'cunts', 'hoe', 'thot', 'pussy',\n",
    "                      'hag', 'hags', 'bimbo', 'bimbos', 'prick', 'pricks']\n",
    "        feat.append(sum(1 for w in derogatory if w in text_lower))\n",
    "        \n",
    "        # flag gender pronouns\n",
    "        total_pronouns = text_lower.count('she') + text_lower.count('her') + text_lower.count('he') + text_lower.count('his') + 1\n",
    "        female_pronouns = text_lower.count('she') + text_lower.count('her')\n",
    "        feat.append(female_pronouns / total_pronouns)\n",
    "        \n",
    "        # flag female oriented words\n",
    "        female_words = ['woman', 'women', 'girl', 'girls', 'female', 'lady']\n",
    "        feat.append(sum(1 for w in female_words if w in text_lower))\n",
    "        \n",
    "        # flag commanding language\n",
    "        commands = ['should', 'must', 'need', 'have to', 'supposed']\n",
    "        feat.append(sum(1 for cmd in commands if cmd in text_lower))\n",
    "        \n",
    "        # flag excessive exclamation marks\n",
    "        feat.append(min(text.count('!'), 3))\n",
    "        \n",
    "        # flag negative adjectives\n",
    "        negative = ['ugly', 'disgusting', 'fat', 'stupid', 'dumb']\n",
    "        feat.append(sum(1 for neg in negative if neg in text_lower))\n",
    "        \n",
    "        # word count (log scaled)\n",
    "        feat.append(np.log1p(len(words)))\n",
    "        \n",
    "        features.append(feat)\n",
    "    \n",
    "    return np.array(features, dtype=float)\n",
    "\n",
    "X_train_custom = extract_custom_features(train_df)\n",
    "X_test_custom = extract_custom_features(test_df)\n",
    "\n",
    "\n",
    "# 4. Combine all features\n",
    "# Convert to sparse matrices for efficiency\n",
    "# Convert to sparse matrix so it can be stacked with TF-IDF\n",
    "glove_train_sparse = csr_matrix(X_train_glove)\n",
    "glove_test_sparse = csr_matrix(X_test_glove)\n",
    "ctx_train_sparse = csr_matrix(X_train_ctx)\n",
    "ctx_test_sparse = csr_matrix(X_test_ctx)\n",
    "custom_train_sparse = csr_matrix(X_train_custom)\n",
    "custom_test_sparse = csr_matrix(X_test_custom)\n",
    "\n",
    "# Stack all features horizontally: TF-IDF + GloVe + Custom\n",
    "X_train_combined = hstack([\n",
    "    X_train_tfidf,\n",
    "    glove_train_sparse,\n",
    "    ctx_train_sparse,\n",
    "    custom_train_sparse\n",
    "])\n",
    "\n",
    "X_test_combined = hstack([\n",
    "    X_test_tfidf,\n",
    "    glove_test_sparse,\n",
    "    ctx_test_sparse,\n",
    "    custom_test_sparse\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7f363",
   "metadata": {},
   "source": [
    "### 3. Models + Training with Training Data Set\n",
    "For our models we decided to use the traditional machine learning models. LogisticRegression, SVC and RandomForest. For all models, we decided to use class_weight=balanced to punish for more sexist or non-sexist statements depending on the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d89884",
   "metadata": {},
   "source": [
    "### Model 1: Logistical Regression\n",
    "\n",
    "We ran each of our features as well as our combined feature trained set, through an ensemble of 5 LogisticRegression models with a variety of logarithmic C values and use soft voting with the VotingClassifier to get our final model which is then ran on the test set and printed out in a classification report and then results are stored for later comparison at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba790136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(x_train_set, y_train):\n",
    "    # Different C values provide diversity, class_weight handles imbalance\n",
    "    lr1 = LogisticRegression(C=0.01, class_weight='balanced' , max_iter=2000, random_state=42)\n",
    "    lr2 = LogisticRegression(C=0.1, class_weight='balanced', max_iter=2000, random_state=43)\n",
    "    lr3 = LogisticRegression(C=1.0, class_weight='balanced', max_iter=2000, random_state=44)\n",
    "    lr4 = LogisticRegression(C=10.0, class_weight='balanced', max_iter=2000, random_state=45)\n",
    "    lr5 = LogisticRegression(C=100, class_weight='balanced', max_iter=2000, random_state=46) \n",
    "\n",
    "    # Soft voting averages the probability predictions from all 5 models\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[('lr1', lr1), ('lr2', lr2), ('lr3', lr3), ('lr4', lr4), ('lr5', lr5)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    ensemble.fit(x_train_set, y_train)\n",
    "    \n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541e6d2",
   "metadata": {},
   "source": [
    "### Model 2: Support Vector Classifier Model\n",
    "Similar to logistical regression, uses C value along with random states to get a variety of results which is then put into an ensemble for voting to create a final prediction model. It is then ran on the test set where the results are stored for later comparison at the end,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_train(x_train_set, y_train):\n",
    "    # Different C values provide diversity, class_weight handles imbalance\n",
    "    svc1 = SVC(C=0.01, class_weight='balanced', probability = True, random_state=42)\n",
    "    svc2 = SVC(C=0.1, class_weight='balanced', probability = True, random_state=43)\n",
    "    svc3 = SVC(C=1.0, class_weight='balanced', probability = True, random_state=44)\n",
    "    svc4 = SVC(C=10.0, class_weight='balanced',probability = True, random_state=45)\n",
    "    svc5 = SVC(C=100, class_weight='balanced',probability = True, random_state=46) \n",
    "\n",
    "    # Soft voting averages the probability predictions from all 5 models\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[('svc1', svc1), ('svc2', svc2), ('svc3', svc3), ('svc4', svc4), ('svc5', svc5)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    ensemble.fit(x_train_set, y_train)\n",
    "\n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eedc3e",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest Classifier\n",
    "Uses the Random Forest Classifier model with multiple random states and put them into an ensemble with soft voting to create a model on the data set. After the ensemble fits the training set, it runs on the test set and displays the results in a classification report where the data is stored for final comparison at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFC_train(x_train_set, y_train):\n",
    "    # Different C values provide diversity, class_weight handles imbalance\n",
    "    rfc1 = RFC( class_weight='balanced',  n_estimators=2000, random_state=42)\n",
    "    rfc2 = RFC( class_weight='balanced',  n_estimators=2000, random_state=43)\n",
    "    rfc3 = RFC( class_weight='balanced',  n_estimators=2000, random_state=44)\n",
    "    rfc4 = RFC( class_weight='balanced',  n_estimators=2000, random_state=45)\n",
    "    rfc5 = RFC( class_weight='balanced', n_estimators=2000, random_state=46) \n",
    "\n",
    "    # Soft voting averages the probability predictions from all 5 models\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[('rfc1', rfc1), ('rfc2', rfc2), ('rfc3', rfc3), ('rfc4', rfc4), ('rfc5', rfc5)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    ensemble.fit(x_train_set, y_train)\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0b358",
   "metadata": {},
   "source": [
    "### 4. Training and Validation + Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df7306",
   "metadata": {},
   "source": [
    "Train Logistical Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ac96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate on TF-IDF features\n",
    "lr_model_tfidf = logistic_regression_train(X_train_tfidf, y_train)\n",
    "# Train and evaluate on Glove features\n",
    "lr_model_glove = logistic_regression_train(X_train_glove, y_train)\n",
    "# Train and evaluate on Contextual Embedding features\n",
    "lr_model_ctx = logistic_regression_train(X_train_ctx, y_train)\n",
    "# Train and evaluate on Custom features\n",
    "lr_model_custom = logistic_regression_train(X_train_custom, y_train)\n",
    "# Train and evaluate on Combined features\n",
    "lr_model_combined =logistic_regression_train(X_train_combined, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b518d34",
   "metadata": {},
   "source": [
    "Train SVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_combined_svc = scaler.fit_transform(X_train_combined.toarray())\n",
    "X_test_combined_svc = scaler.transform(X_test_combined.toarray())\n",
    "X_train_tfidf_svc = scaler.fit_transform(X_train_tfidf.toarray())\n",
    "X_test_tfidf_svc = scaler.transform(X_test_tfidf.toarray())\n",
    "X_train_glove_svc = scaler.fit_transform(X_train_glove)\n",
    "X_test_glove_svc = scaler.transform(X_test_glove)\n",
    "X_train_ctx_svc = scaler.fit_transform(X_train_ctx)\n",
    "X_test_ctx_svc = scaler.transform(X_test_ctx)\n",
    "X_train_custom_svc = scaler.fit_transform(X_train_custom)\n",
    "X_test_custom_svc = scaler.transform(X_test_custom)\n",
    "\n",
    "# Train and evaluate on TF-IDF features\n",
    "svc_tfidf_model = SVC_train(X_train_tfidf_svc, y_train )\n",
    "# Train and evaluate on Glove features\n",
    "svc_glove_model = SVC_train(X_train_glove_svc,  y_train)\n",
    "# Train and evaluate on Contextual Embedding features\n",
    "svc_ctx_model = SVC_train(X_train_ctx_svc, y_train)\n",
    "# Train and evaluate on Custom features\n",
    "svc_custom_model = SVC_train(X_train_custom_svc, y_train)\n",
    "# Train and evaluate on Combined features\n",
    "svc_combined_model = SVC_train(X_train_combined_svc, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e22830",
   "metadata": {},
   "source": [
    "Train Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ee457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RFC on TF-IDF features\n",
    "rfc_tfidf_model = RFC_train(X_train_tfidf, y_train)\n",
    "# Train RFC on Glove features\n",
    "rfc_glove_model = RFC_train(X_train_glove, y_train)\n",
    "# Train RFC on Contextual Embedding features\n",
    "rfc_ctx_model = RFC_train(X_train_ctx, y_train)\n",
    "# Train RFC on Custom features\n",
    "rfc_custom_model = RFC_train(X_train_custom, y_train)\n",
    "# Train RFC on Combined features\n",
    "rfc_combined_model = RFC_train(X_train_combined, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149cab6",
   "metadata": {},
   "source": [
    "### 5. Evaluate models\n",
    "Uses model with test set and an optimal threshold calculated from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4dd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, optimal_threshold):\n",
    "\n",
    "    # Get probability predictions (probability of sexist class)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "    y_pred_labels = le.inverse_transform(y_pred)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    return y_pred_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_threshold(model, x_train, y_train):\n",
    "    # Tune threshold on training set to maximize accuracy\n",
    "    y_proba = model.predict_proba(x_train)[:, 1]\n",
    "    best_threshold = 0.5\n",
    "    best_acc = accuracy_score(y_train, (y_proba >= best_threshold).astype(int))\n",
    "\n",
    "    for threshold in np.arange(0.25, 0.75, 0.01):\n",
    "        acc = accuracy_score(y_train, (y_proba >= threshold).astype(int))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_threshold = float(threshold)\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136943d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all LR models\n",
    "print(\"Evaluating Logistic Regression Models:\")\n",
    "lr_tfidf_preds = evaluate_model(lr_model_tfidf, X_test_tfidf, y_test, tune_threshold(lr_model_tfidf, X_train_tfidf, y_train))\n",
    "lr_glove_preds = evaluate_model(lr_model_glove, X_test_glove, y_test,  tune_threshold(lr_model_glove, X_train_glove, y_train))\n",
    "lr_ctx_preds = evaluate_model(lr_model_ctx, X_test_ctx, y_test, tune_threshold(lr_model_ctx, X_train_ctx, y_train))\n",
    "lr_custom_preds = evaluate_model(lr_model_custom, X_test_custom, y_test, tune_threshold(lr_model_custom, X_train_custom, y_train))\n",
    "lr_combined_preds = evaluate_model(lr_model_combined, X_test_combined, y_test, tune_threshold(lr_model_combined, X_train_combined, y_train))\n",
    "# Evaluate all SVC models\n",
    "print(\"Evaluating SVC Models:\")\n",
    "svc_tfidf_preds = evaluate_model(svc_tfidf_model, X_test_tfidf_svc, y_test, tune_threshold(svc_tfidf_model, X_train_tfidf_svc, y_train))\n",
    "svc_glove_preds = evaluate_model(svc_glove_model, X_test_glove_svc  , y_test, tune_threshold(svc_glove_model, X_train_glove_svc, y_train) )\n",
    "svc_ctx_preds = evaluate_model(svc_ctx_model, X_test_ctx_svc, y_test, tune_threshold(svc_ctx_model, X_train_ctx_svc, y_train))\n",
    "svc_custom_preds = evaluate_model(svc_custom_model, X_test_custom_svc, y_test, tune_threshold(svc_custom_model, X_train_custom_svc, y_train))\n",
    "svc_combined_preds = evaluate_model(svc_combined_model, X_test_combined_svc, y_test, tune_threshold(svc_combined_model, X_train_combined_svc, y_train))\n",
    "# Evaluate all RFC models\n",
    "print(\"Evaluating RFC Models:\")\n",
    "rfc_tfidf_preds = evaluate_model(rfc_tfidf_model, X_test_tfidf, y_test, tune_threshold(rfc_tfidf_model, X_train_tfidf, y_train))\n",
    "rfc_glove_preds = evaluate_model(rfc_glove_model, X_test_glove, y_test, tune_threshold(rfc_glove_model, X_train_glove, y_train))\n",
    "rfc_ctx_preds = evaluate_model(rfc_ctx_model, X_test_ctx, y_test, tune_threshold(rfc_ctx_model, X_train_ctx, y_train))\n",
    "rfc_custom_preds = evaluate_model(rfc_custom_model, X_test_custom, y_test, tune_threshold(rfc_custom_model, X_train_custom, y_train))\n",
    "rfc_combined_preds = evaluate_model(rfc_combined_model, X_test_combined, y_test, tune_threshold(rfc_combined_model, X_train_combined, y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f395ff3",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "This section has a function that grabs the data from each of the results of the models and stores it into a result array which is tthen displayed as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67cb180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ensure y_true and y_pred use the same label types (numeric) before computing metrics.\n",
    "def extract_full_metrics(y_true, y_pred, feature_name, model_name, labels=[1, 0]):\n",
    "    y_true_arr = np.array(y_true)\n",
    "    y_pred_arr = np.array(y_pred)\n",
    "\n",
    "    # check if y_true and y_pred are strings, convert to numeric labels if so\n",
    "    if y_true_arr.dtype.kind in (\"U\", \"S\", \"O\"):\n",
    "        y_true_arr = le.transform(y_true_arr)\n",
    "    if y_pred_arr.dtype.kind in (\"U\", \"S\", \"O\"):\n",
    "        y_pred_arr = le.transform(y_pred_arr)\n",
    "\n",
    "    y_true_arr = y_true_arr.astype(int)\n",
    "    y_pred_arr = y_pred_arr.astype(int)\n",
    "\n",
    "    precisions, recalls, f1s, _ = precision_recall_fscore_support(\n",
    "        y_true_arr, y_pred_arr, labels=labels, zero_division=0\n",
    "    )\n",
    "\n",
    "    weighted_p = precision_score(y_true_arr, y_pred_arr, average=\"weighted\", zero_division=0)\n",
    "    weighted_r = recall_score(y_true_arr, y_pred_arr, average=\"weighted\", zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true_arr, y_pred_arr, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"Feature + Model\": f\"{feature_name} + {model_name}\",\n",
    "        \"Sexist (P)\": float(precisions[0]),\n",
    "        \"Sexist (R)\": float(recalls[0]),\n",
    "        \"Sexist (F1)\": float(f1s[0]),\n",
    "        \"Non-Sexist (P)\": float(precisions[1]),\n",
    "        \"Non-Sexist (R)\": float(recalls[1]),\n",
    "        \"Non-Sexist (F1)\": float(f1s[1]),\n",
    "        \"Weighted (P)\": float(weighted_p),\n",
    "        \"Weighted (R)\": float(weighted_r),\n",
    "        \"Weighted (F1)\": float(weighted_f1),\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "# Collect results for all models\n",
    "results.append(extract_full_metrics(y_train, lr_tfidf_preds, \"TF-IDF\", \"Logistic Regression\"))\n",
    "results.append(extract_full_metrics(y_train, lr_glove_preds, \"GloVe\", \"Logistic Regression\"))\n",
    "results.append(extract_full_metrics(y_train, lr_ctx_preds, \"Contextual Embeddings\", \"Logistic Regression\"))\n",
    "results.append(extract_full_metrics(y_train, lr_custom_preds, \"Custom Features\", \"Logistic Regression\"))\n",
    "results.append(extract_full_metrics(y_train, lr_combined_preds, \"Combined Features\", \"Logistic Regression\")) \n",
    "results.append(extract_full_metrics(y_train, svc_tfidf_preds, \"TF-IDF\", \"SVC\"))\n",
    "results.append(extract_full_metrics(y_train, svc_glove_preds, \"GloVe\", \"SVC\"))\n",
    "results.append(extract_full_metrics(y_train, svc_ctx_preds, \"Contextual Embeddings\", \"SVC\"))\n",
    "results.append(extract_full_metrics(y_train, svc_custom_preds, \"Custom Features\", \"SVC\"))\n",
    "results.append(extract_full_metrics(y_train, svc_combined_preds, \"Combined Features\", \"SVC\")) \n",
    "results.append(extract_full_metrics(y_train, rfc_tfidf_preds, \"TF-IDF\", \"RFC\"))\n",
    "results.append(extract_full_metrics(y_train, rfc_glove_preds, \"GloVe\", \"RFC\"))\n",
    "results.append(extract_full_metrics(y_train, rfc_ctx_preds, \"Contextual Embeddings\", \"RFC\"))\n",
    "results.append(extract_full_metrics(y_train, rfc_custom_preds, \"Custom Features\", \"RFC\"))\n",
    "results.append(extract_full_metrics(y_train, rfc_combined_preds, \"Combined Features\", \"RFC\"))\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "display(results_df)\n",
    "print(\"\\nBest Performance Summary:\")\n",
    "best_row = results_df.loc[results_df[\"Weighted (F1)\"].idxmax()]\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Summary.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)\n",
    "\n",
    "summary = \"\"\"Summary of Best Model:\n",
    "\n",
    "Data Preprocessing: \n",
    "    Rationale: Since punctuation and hashtags and mentions can sound aggressive or \n",
    "    feel like patterns showing sexism, minimal preprocessing was done to keep\n",
    "    the original features. URLs were removed as they are unknown unless we went in the site and checked\n",
    "    \n",
    "    Steps:\n",
    "    - Lowercase to keep things consistent \n",
    "    - Took out URLs \n",
    "    - Whitespace normalization\n",
    "\n",
    "Feature Engineering: \n",
    "    We combined three feature types to capture different patterns of sexism:\n",
    "    \n",
    "    1. TF-IDF (1800 features): \n",
    "       - Captures word importance via inverse document frequency\n",
    "       - Bigrams (1,2) to capture common sexist phrases \n",
    "       - Chi-squared feature choosing to keep most discriminative terms\n",
    "       - min_df=3: this means a word must appear in 3 documents to be counted, ignores typos and such\n",
    "       - max_df=0.85: removes overly common words that don't indicate anything\n",
    "       - sublinear_tf=True: log scaling keeps extremely common words from dominating\n",
    "    \n",
    "    2. GloVe embeddings (100 features): \n",
    "       - Makes synonyms match in importance as vectors\n",
    "       - Mean pooling combines word vectors into sentence vectors\n",
    "       - Complements TF-IDF by understanding synonyms and context\n",
    "    \n",
    "    3. Custom features (8 features): \n",
    "       Features that matter a lot that the first two were missing:\n",
    "       - Derogatory word count\n",
    "       - Female pronoun to total pronoun ratio\n",
    "       - Checking for woman/women/girl/female/lady to weigh more\n",
    "       - Commanding language: should/must/need/have to\n",
    "       - Intensity markers: exclamation marks \n",
    "       - Negative descriptors: insulting adjectives \n",
    "       - Text length as a feature, scaled logarithmically\n",
    "       - All-caps word count\n",
    "    \n",
    "    Final feature vector: 1908 dimensions (1800 + 100 + 8)\n",
    "\n",
    "Model Selection and Architecture: \n",
    "    Ensemble approach (merging several models) to reduce overfitting and make generalization better:\n",
    "    \n",
    "    - Base model: Logistic Regression \n",
    "    - Ensemble: 4 models with different regularization strengths\n",
    "      * C values: {2.0, 3.0, 4.0, 5.0} for diversity in decisions\n",
    "      * Lower C = stronger regularization so less overfitting\n",
    "      * Higher C = weaker regularization so more nuance \n",
    "    - Average probability predictions \n",
    "    - Class weight {0: 1, 1: 1.5}: Penalizes misclassifying sexist examples more\n",
    "\n",
    "Training and Validation: \n",
    "    - Stratified 5-fold cross-validation \n",
    "    - Cross-validation during hyperparameter search prevented overfitting\n",
    "    - Threshold optimization: Tested 0.40-0.65 to find best threshold\n",
    "      *\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd61f80",
   "metadata": {},
   "source": [
    "### 6. Use of Generative AI\n",
    "For this project, in order to research libraries we were unfamiliar with such as TF-IDF, GloVe Embeddings, Sentence-Transformers, and Voting Classifiers, AI was used to learn about these libraries. AI was also used for large level ideas on possible improvements for our Feature Engineering such as explaining what TF-IDF Vectorizer parameters would further improve our feature engineering, what GloVe embeddings to use and how to use it, also used to suggest potential flag words in our custom_features. All code for text preprocessing, models, and the evaluation was manually written. Feature engineering section used AI for selection of the model \"all-mpnet-base-v2\" for Sentence Transformers. [Insert Other Use of AI here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
